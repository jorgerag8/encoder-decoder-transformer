{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from fastprogress import master_bar, progress_bar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingset = pd.DataFrame(pd.read_csv(\"/Users/jorgerag/Documents/UCSD/courses/Winter23/COGS181/final_project/data/photometry_analog/processed/train_data.csv\"))\n",
    "testset = pd.DataFrame(pd.read_csv(\"/Users/jorgerag/Documents/UCSD/courses/Winter23/COGS181/final_project/data/photometry_analog/processed/test_data.csv\"))\n",
    "trainingset = trainingset[[\"subject\", \"day\", \"lp_met\", \"gcamp_lp_per_sec\"]]\n",
    "testset = testset[[\"subject\", \"day\", \"lp_met\", \"gcamp_lp_per_sec\"]]\n",
    "# drop infs\n",
    "trainingset = trainingset[~trainingset.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
    "testset = testset[~testset.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
    "\n",
    "# Get dimensions down\n",
    "#trainingset[\"gcamp_lp_per_sec\"] = trainingset[\"gcamp_lp_per_sec\"].round(-2).astype(\"float32\")\n",
    "#testset[\"gcamp_lp_per_sec\"] = testset[\"gcamp_lp_per_sec\"].round(-2).astype(\"float32\")\n",
    "\n",
    "## Divide dataset by trials\n",
    "gb = trainingset.groupby([\"subject\", \"day\"])\n",
    "trials = [gb.get_group(x) for x in gb.groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial1 = trials[0]\n",
    "trainX = []\n",
    "trainy = []\n",
    "validX = []\n",
    "validy = []\n",
    "for elem in trials:\n",
    "    train_X, valid_X, train_y, valid_y = train_test_split(elem[\"gcamp_lp_per_sec\"], elem[\"lp_met\"], test_size=0.5, shuffle=False)\n",
    "    trainX.append(train_X)\n",
    "    trainy.append(train_y)\n",
    "    validX.append(valid_X)\n",
    "    validy.append(valid_y)\n",
    "\n",
    "def find_max_list(list):\n",
    "    list_len = [len(i) for i in list]\n",
    "    return max(list_len)\n",
    "\n",
    "max_list = find_max_list(validX)\n",
    "\n",
    "def pad_zeros(list):\n",
    "    new_list = []\n",
    "    for elem in list:\n",
    "        new_elem = np.pad(elem, (0, max_list + 1 - len(elem)), 'constant')\n",
    "        new_list.append(new_elem)\n",
    "    return new_list\n",
    "\n",
    "trainX = pad_zeros(trainX)\n",
    "validX = pad_zeros(validX)\n",
    "trainy = pad_zeros(trainy)\n",
    "validy = pad_zeros(validy)\n",
    "\n",
    "seq_length = len(trainX[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = Variable(torch.Tensor(trainX))\n",
    "validX = Variable(torch.Tensor(validX))\n",
    "trainy = Variable(torch.Tensor(trainy))\n",
    "validy = Variable(torch.Tensor(validy))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, seq_len, n_features, embedding_dim=64):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.seq_len, self.n_features = seq_len, n_features\n",
    "        self.embedding_dim, self.hidden_dim = embedding_dim,  embedding_dim\n",
    "        self.num_layers = 3\n",
    "        self.rnn1 = nn.LSTM(\n",
    "          input_size=n_features,\n",
    "          hidden_size=self.hidden_dim,\n",
    "          num_layers=3,\n",
    "          batch_first=True,\n",
    "          dropout = 0.35\n",
    "        )\n",
    "   \n",
    "    def forward(self, x):\n",
    "       \n",
    "        x = x.reshape((1, self.seq_len, self.n_features))\n",
    "        \n",
    "        h_1 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_dim).to(device))\n",
    "         \n",
    "        \n",
    "        c_1 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_dim).to(device))\n",
    "              \n",
    "        x, (hidden, cell) = self.rnn1(x,(h_1, c_1))\n",
    "        \n",
    "        \n",
    "        #return hidden_n.reshape((self.n_features, self.embedding_dim))\n",
    "        return x, hidden , cell "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = nn.Linear((enc_hid_dim ) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        \n",
    "       \n",
    "        hidden = hidden[2:3,:,:]\n",
    "        \n",
    "        #print(\"hidden size is\",hidden.size())\n",
    "        \n",
    "        \n",
    "        \n",
    "        #repeat decoder hidden state src_len times\n",
    "        #hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        hidden = hidden.repeat(1, src_len, 1)\n",
    "     \n",
    "        \n",
    "        #encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #print(\"encode_outputs size after permute is:\",encoder_outputs.size())\n",
    "        \n",
    "        \n",
    "        #hidden = [batch size, src len, dec hid dim]\n",
    "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
    "        \n",
    "        #energy = [batch size, src len, dec hid dim]\n",
    "\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        \n",
    "        #attention= [batch size, src len]\n",
    "        \n",
    "        \n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, seq_len, input_dim=64, n_features=1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.seq_len, self.input_dim = seq_len, input_dim\n",
    "        self.hidden_dim, self.n_features =  input_dim, n_features\n",
    "        \n",
    "        self.rnn1 = nn.LSTM(\n",
    "          input_size=1,\n",
    "          hidden_size=input_dim,\n",
    "          num_layers=3,\n",
    "          batch_first=True,\n",
    "          dropout = 0.35\n",
    "        )\n",
    "        \n",
    "        \n",
    "      \n",
    "        self.output_layer = nn.Linear(self.hidden_dim, n_features)\n",
    "\n",
    "    def forward(self, x,input_hidden,input_cell):\n",
    "       \n",
    "       \n",
    "        x = x.reshape((1,1,1))\n",
    "        \n",
    "        \n",
    "     \n",
    "\n",
    "        x, (hidden_n, cell_n) = self.rnn1(x,(input_hidden,input_cell))\n",
    "    \n",
    "        x = self.output_layer(x)\n",
    "        return x, hidden_n, cell_n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attention decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, seq_len,attention, input_dim=64, n_features=1,encoder_hidden_state = 512):\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "\n",
    "        self.seq_len, self.input_dim = seq_len, input_dim\n",
    "        self.hidden_dim, self.n_features =  input_dim, n_features\n",
    "        self.attention = attention \n",
    "        \n",
    "        self.rnn1 = nn.LSTM(\n",
    "          #input_size=1,\n",
    "          input_size= encoder_hidden_state + 1,  # Encoder Hidden State + One Previous input\n",
    "          hidden_size=input_dim,\n",
    "          num_layers=3,\n",
    "          batch_first=True,\n",
    "          dropout = 0.35\n",
    "        )\n",
    "        \n",
    "        \n",
    "      \n",
    "        self.output_layer = nn.Linear(self.hidden_dim * 2 , n_features)\n",
    "\n",
    "    def forward(self, x,input_hidden,input_cell,encoder_outputs):\n",
    "       \n",
    "        a = self.attention(input_hidden, encoder_outputs)\n",
    "        \n",
    "        a = a.unsqueeze(1)\n",
    "        \n",
    "        #a = [batch size, 1, src len]\n",
    "        \n",
    "        #encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
    "        \n",
    "      \n",
    "        \n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        \n",
    "        \n",
    "     \n",
    "        x = x.reshape((1,1,1))\n",
    "       \n",
    "        \n",
    "        \n",
    "        rnn_input = torch.cat((x, weighted), dim = 2)\n",
    "       \n",
    "\n",
    "        #x, (hidden_n, cell_n) = self.rnn1(x,(input_hidden,input_cell))\n",
    "        x, (hidden_n, cell_n) = self.rnn1(rnn_input,(input_hidden,input_cell))\n",
    "        \n",
    "        output = x.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        x = self.output_layer(torch.cat((output, weighted), dim = 1))\n",
    "        return x, hidden_n, cell_n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "\n",
    "    def __init__(self, seq_len, n_features, embedding_dim=64, output_length = len(validy[0])):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        \n",
    "        self.encoder = Encoder(seq_len, n_features, embedding_dim).to(device)\n",
    "        self.attention = Attention(512,512)\n",
    "        self.output_length = output_length\n",
    "        self.decoder = AttentionDecoder(seq_len, self.attention, embedding_dim, n_features).to(device)\n",
    "        \n",
    "\n",
    "    def forward(self,x, prev_y):\n",
    "        \n",
    "        encoder_output,hidden,cell = self.encoder(x)\n",
    "         \n",
    "        #Prepare place holder for decoder output\n",
    "        targets_ta = []\n",
    "        #prev_output become the next input to the LSTM cell\n",
    "        prev_output = prev_y\n",
    "        \n",
    "        #itearate over LSTM - according to the required output days\n",
    "        for out_days in range(self.output_length) :\n",
    "        \n",
    "            prev_x,prev_hidden,prev_cell = self.decoder(prev_output,hidden,cell,encoder_output)\n",
    "            hidden,cell = prev_hidden,prev_cell\n",
    "            prev_output = prev_x\n",
    "            \n",
    "            targets_ta.append(prev_x.reshape(1))\n",
    "           \n",
    "            \n",
    "        \n",
    "        \n",
    "        targets = torch.stack(targets_ta)\n",
    "\n",
    "        return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 1\n",
    "model = Seq2Seq(seq_length, n_features, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (rnn1): LSTM(1, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
       "  )\n",
       "  (attention): Attention(\n",
       "    (attn): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (v): Linear(in_features=512, out_features=1, bias=False)\n",
       "  )\n",
       "  (decoder): AttentionDecoder(\n",
       "    (attention): Attention(\n",
       "      (attn): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
       "    )\n",
       "    (rnn1): LSTM(513, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
       "    (output_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (rnn1): LSTM(1, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
       "  )\n",
       "  (attention): Attention(\n",
       "    (attn): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (v): Linear(in_features=512, out_features=1, bias=False)\n",
       "  )\n",
       "  (decoder): AttentionDecoder(\n",
       "    (attention): Attention(\n",
       "      (attn): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
       "    )\n",
       "    (rnn1): LSTM(513, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
       "    (output_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=4e-3,weight_decay=1e-5)\n",
    "criterion = torch.nn.MSELoss().to(device) \n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 5e-3, eta_min=1e-8, last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, TrainX,Trainy,ValidX,Validy,seq_length, n_epochs):\n",
    "  \n",
    "    history = dict(train=[], val=[])\n",
    "\n",
    "    #best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 10000.0\n",
    "    mb = master_bar(range(1, n_epochs + 1))\n",
    "\n",
    "    for epoch in mb:\n",
    "        model = model.train()\n",
    "\n",
    "        train_losses = []\n",
    "        for i in progress_bar(range(TrainX.size()[0]),parent=mb):\n",
    "            seq_inp = TrainX[i,:]\n",
    "            seq_true = Trainy[i,:]\n",
    "           \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            \n",
    "            seq_pred = model(seq_inp,seq_inp[seq_length-1:seq_length])\n",
    "            \n",
    "            \n",
    "            loss = criterion(seq_pred, seq_true)\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        val_losses = []\n",
    "        model = model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in progress_bar(range(validX.size()[0]), parent=mb):\n",
    "                seq_inp = ValidX[i,:]\n",
    "                seq_true = Validy[i,:]\n",
    "        \n",
    "                seq_pred = model(seq_inp,seq_inp[seq_length-1:seq_length])\n",
    "               \n",
    "\n",
    "                loss = criterion(seq_pred, seq_true)\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "        train_loss = np.mean(train_losses)\n",
    "        val_loss = np.mean(val_losses)\n",
    "\n",
    "        history['train'].append(train_loss)\n",
    "        history['val'].append(val_loss)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "            print(\"saved best model epoch:\",epoch,\"val loss is:\",val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch}: train loss {train_loss} val loss {val_loss}')\n",
    "        scheduler.step()\n",
    "    #model.load_state_dict(best_model_wts)\n",
    "    return model.eval(), history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, history = train_model(\n",
    "  model,\n",
    "  trainX, trainy,\n",
    "  validX, validy,\n",
    "  seq_length,\n",
    "  n_epochs=5, ## Training only on 30 epochs to save GPU time \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (rnn1): LSTM(1, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
       "  )\n",
       "  (attention): Attention(\n",
       "    (attn): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (v): Linear(in_features=512, out_features=1, bias=False)\n",
       "  )\n",
       "  (decoder): AttentionDecoder(\n",
       "    (attention): Attention(\n",
       "      (attn): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
       "    )\n",
       "    (rnn1): LSTM(513, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
       "    (output_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    seq_inp = validX[0]\n",
    "    \n",
    "    seq_pred = model(seq_inp,seq_inp[seq_length-1:seq_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0212],\n",
       "        [-0.0110],\n",
       "        [-0.1040],\n",
       "        [-0.1173],\n",
       "        [-0.1197],\n",
       "        [-0.1202],\n",
       "        [-0.1203],\n",
       "        [-0.1202],\n",
       "        [-0.1201],\n",
       "        [-0.1199],\n",
       "        [-0.1198],\n",
       "        [-0.1196],\n",
       "        [-0.1195],\n",
       "        [-0.1194],\n",
       "        [-0.1194],\n",
       "        [-0.1194],\n",
       "        [-0.1193],\n",
       "        [-0.1192],\n",
       "        [-0.1187],\n",
       "        [-0.1175],\n",
       "        [-0.1166],\n",
       "        [-0.1165],\n",
       "        [-0.1164],\n",
       "        [-0.1164],\n",
       "        [-0.1164],\n",
       "        [-0.1165],\n",
       "        [-0.1165],\n",
       "        [-0.1165],\n",
       "        [-0.1166],\n",
       "        [-0.1170],\n",
       "        [-0.1177],\n",
       "        [-0.1180],\n",
       "        [-0.1180],\n",
       "        [-0.1180],\n",
       "        [-0.1180],\n",
       "        [-0.1180],\n",
       "        [-0.1180],\n",
       "        [-0.1180],\n",
       "        [-0.1180],\n",
       "        [-0.1180],\n",
       "        [-0.1180],\n",
       "        [-0.1180],\n",
       "        [-0.1180],\n",
       "        [-0.1180],\n",
       "        [-0.1179],\n",
       "        [-0.1179],\n",
       "        [-0.1178],\n",
       "        [-0.1178],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1178],\n",
       "        [-0.1178],\n",
       "        [-0.1178],\n",
       "        [-0.1178],\n",
       "        [-0.1178],\n",
       "        [-0.1178],\n",
       "        [-0.1178],\n",
       "        [-0.1178],\n",
       "        [-0.1178],\n",
       "        [-0.1178],\n",
       "        [-0.1178],\n",
       "        [-0.1178],\n",
       "        [-0.1178],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1178],\n",
       "        [-0.1178],\n",
       "        [-0.1179],\n",
       "        [-0.1179],\n",
       "        [-0.1179],\n",
       "        [-0.1179],\n",
       "        [-0.1179],\n",
       "        [-0.1179],\n",
       "        [-0.1179],\n",
       "        [-0.1179],\n",
       "        [-0.1178],\n",
       "        [-0.1178],\n",
       "        [-0.1178],\n",
       "        [-0.1178],\n",
       "        [-0.1178],\n",
       "        [-0.1178],\n",
       "        [-0.1178],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177],\n",
       "        [-0.1177]])"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
       "        1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
       "        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
       "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "89946d4749d924484f97ac4b378b806e1fec06ce290fedea0f13c58dafe8aaf8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
